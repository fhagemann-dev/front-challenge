{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d5c8534-41ee-4084-b676-6b0c0f292f5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b25ead9d-da2c-4f0f-952f-fd420e03086e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Setup\n",
    "This notebook implements a robust Weather ETL Pipeline designed to process, store, and analyze weather data. The architecture leverages Azure Databricks and a Lakehouse architecture, ensuring scalability, efficiency, and flexibility for future extensions. The pipeline processes weather data from the National Weather Service API, stores it in a Delta table, and answers two specific analytical queries using SQL.\n",
    "\n",
    "**Compute:** I used Azure Databricks as the processing engine to leverage its distributed computing capabilities and seamless integration with Azure services.\n",
    "\n",
    "**Storage:** I chose Azure Blob Storage as the data lake to store raw and intermediate data in Parquet format, which ensures efficient storage and querying.\n",
    "\n",
    "**Data Management:** I utilized Delta Tables in the Unity Catalog for structured, ACID-compliant storage with centralized governance, making it easy to query and maintain high data quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f403d63d-228b-4cb5-b46b-a86b78ac061e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Libraries\n",
    "The notebook imports the necessary libraries, such as requests for API interaction and PySpark for scalable data processing. These libraries were chosen to ensure efficiency, scalability, and compatibility with the Databricks environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "465757fb-b2c2-4d7e-af39-129e7fa88a0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import logging\n",
    "from typing import Any, Dict, List, Optional\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "from pyspark.sql.functions import col, to_timestamp, current_timestamp, date_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70b30cf6-8d7b-4219-ab77-5b0cccea2f8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "I used a Logger because it provides a robust and structured way to monitor and debug pipelines, ensuring better traceability and maintainability compared to simple print statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9bdb05a-cbd2-41e0-97ec-23b0f8f8907d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"WeatherDataPipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d830d850-35a8-4b34-805a-f8fb1a65aeea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Constants\n",
    "Constants are defined for API endpoints, table names, and configurations. Using constants ensures the code is maintainable and easy to adapt to future changes, such as adding new stations or modifying the database structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d27de89-f9af-4bfc-871f-26150ce54a00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "HEADERS = {\"User-Agent\": \"(myweatherapp.com, contact@myweatherapp.com)\"}\n",
    "STATION_ID = \"0128W\"\n",
    "BASE_URL = \"https://api.weather.gov\"\n",
    "OBSERVATIONS_ENDPOINT = f\"{BASE_URL}/stations/{STATION_ID}/observations\"\n",
    "STATION_ENDPOINT = f\"{BASE_URL}/stations/{STATION_ID}\"\n",
    "CATALOG_NAME = \"company_data\"\n",
    "SCHEMA_NAME = \"weather\"\n",
    "TABLE_NAME = \"station_observations\"\n",
    "FULL_TABLE_NAME = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{TABLE_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6bc1b1a-86df-4531-b71e-094bc85717b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Functions\n",
    "\n",
    "**Data Fetching:** The fetch_weather_data_last_7_days function retrieves data for the last 7 days. Filtering is handled in Spark, ensuring performance and scalability for large datasets.\n",
    "\n",
    "**Data Transformation:** The transform_data_to_dataframe function converts raw API data into a structured Spark DataFrame, processing timestamps and rounding numeric values for consistency.\n",
    "\n",
    "**Data Storage:** The upsert_to_table function implements a MERGE operation to prevent duplicate records during subsequent pipeline runs. This ensures data integrity, especially when re-running the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74de18cc-e015-4a54-ab9d-1f07b70baad5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "def fetch_station_metadata(station_id: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fetch metadata for a given station, including timezone.\n",
    "    \n",
    "    Args:\n",
    "        station_id (str): The station ID to fetch metadata for.\n",
    "        \n",
    "    Returns:\n",
    "        Optional[Dict[str, Any]]: A dictionary containing metadata or None if unavailable.\n",
    "    \"\"\"\n",
    "    station_url = f\"{BASE_URL}/stations/{station_id}\"\n",
    "    try:\n",
    "        response = requests.get(station_url, headers=HEADERS)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            return {\n",
    "                \"station_id\": station_id,\n",
    "                \"station_name\": data.get(\"properties\", {}).get(\"name\", None),\n",
    "                \"timezone\": data.get(\"properties\", {}).get(\"timeZone\", None)\n",
    "            }\n",
    "        else:\n",
    "            logger.warning(f\"Failed to fetch metadata for station {station_id}. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching metadata for station {station_id}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0817dcf2-0685-4ab5-9742-482fcae973c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "def fetch_weather_data(station_id: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fetch weather observation data for a given station.\n",
    "    \n",
    "    Args:\n",
    "        station_id (str): The station ID to fetch data for.\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: A list of observation dictionaries.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(OBSERVATIONS_ENDPOINT, headers=HEADERS)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            return data.get(\"features\", [])\n",
    "        else:\n",
    "            logger.warning(f\"Failed to fetch observations for station {station_id}. Status code: {response.status_code}\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching observations for station {station_id}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d35d6c19-6435-42af-90a3-f71e7b94eab3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "def transform_data_to_dataframe(observations: List[Dict[str, Any]], metadata: Dict[str, Any]) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transform observation data and metadata into a PySpark DataFrame.\n",
    "\n",
    "    Args:\n",
    "        observations (List[Dict[str, Any]]): A list of observation dictionaries.\n",
    "        metadata (Dict[str, Any]): Metadata containing timezone and station details.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A PySpark DataFrame.\n",
    "    \"\"\"\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField(\"station_id\", StringType(), True),\n",
    "            StructField(\"station_name\", StringType(), True),\n",
    "            StructField(\"timezone\", StringType(), True),\n",
    "            StructField(\"latitude\", DoubleType(), True),\n",
    "            StructField(\"longitude\", DoubleType(), True),\n",
    "            StructField(\"timestamp_str\", StringType(), True),  # Store as String initially\n",
    "            StructField(\"temperature\", DoubleType(), True),\n",
    "            StructField(\"wind_speed\", DoubleType(), True),\n",
    "            StructField(\"humidity\", DoubleType(), True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    def safe_round(value: Any, decimals: int = 2) -> Optional[float]:\n",
    "        return round(float(value), decimals) if value is not None else None\n",
    "\n",
    "    records = []\n",
    "    for observation in observations:\n",
    "        props = observation.get(\"properties\", {})\n",
    "        coords = observation.get(\"geometry\", {}).get(\"coordinates\", [None, None])\n",
    "        records.append(\n",
    "            {\n",
    "                \"station_id\": metadata[\"station_id\"],\n",
    "                \"station_name\": metadata[\"station_name\"],\n",
    "                \"timezone\": metadata[\"timezone\"],\n",
    "                \"latitude\": coords[1],\n",
    "                \"longitude\": coords[0],\n",
    "                \"timestamp_str\": props.get(\"timestamp\"),  # Keep as raw ISO-8601 string\n",
    "                \"temperature\": safe_round(props.get(\"temperature\", {}).get(\"value\")),\n",
    "                \"wind_speed\": safe_round(props.get(\"windSpeed\", {}).get(\"value\")),\n",
    "                \"humidity\": safe_round(props.get(\"relativeHumidity\", {}).get(\"value\")),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df = spark.createDataFrame(records, schema=schema)\n",
    "    # Convert ISO-8601 string to TimestampType, and drop the original\n",
    "    df = df.withColumn(\"timestamp\", to_timestamp(col(\"timestamp_str\")))\n",
    "    df = df.drop(\"timestamp_str\")\n",
    "    # Filter for only the last 7 days\n",
    "    df = df.filter(col(\"timestamp\") >= date_sub(current_timestamp(), 7))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8157bae-6f21-471e-95a0-18a6b25eff6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "def upsert_to_table(dataframe: DataFrame, table_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Upsert data into a Unity Catalog table using MERGE.\n",
    "    \n",
    "    Args:\n",
    "        dataframe (DataFrame): The DataFrame to upsert.\n",
    "        table_name (str): The table name.\n",
    "    \"\"\"\n",
    "    dataframe.createOrReplaceTempView(\"temp_observations\")\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            station_id STRING,\n",
    "            station_name STRING,\n",
    "            timezone STRING,\n",
    "            latitude DOUBLE,\n",
    "            longitude DOUBLE,\n",
    "            timestamp TIMESTAMP,\n",
    "            temperature DOUBLE,\n",
    "            wind_speed DOUBLE,\n",
    "            humidity DOUBLE\n",
    "        )\n",
    "        USING delta\n",
    "    \"\"\")\n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO {table_name} target\n",
    "        USING temp_observations source\n",
    "        ON target.station_id = source.station_id AND target.timestamp = source.timestamp\n",
    "        WHEN MATCHED THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0959f4d5-e902-45a2-af5c-43356b6e29af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SQL Queries\n",
    "The pipeline includes two SQL queries to compute the required metrics:\n",
    "1. Average Observed Temperature for Last Week (Mon-Sun):\n",
    "This query calculates the average temperature for the previous week using the date_trunc function to ensure strict Mon-Sun boundaries.\n",
    "\n",
    "\n",
    "2. Maximum Wind Speed Change:\n",
    "This query uses a LAG window function to compute differences between consecutive observations and finds the maximum change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0f00596-65ed-4b22-a425-7616bf76b6ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_average_temperature_last_week(full_table_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Calculate the average observed temperature for the last week (Mon-Sun).\n",
    "    \n",
    "    Args:\n",
    "        full_table_name (str): The fully qualified table name (e.g., \"catalog.schema.table\").\n",
    "        \n",
    "    Returns:\n",
    "        None: Displays the results directly.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "        SELECT \n",
    "            AVG(temperature) AS avg_temperature\n",
    "        FROM {full_table_name}\n",
    "        WHERE \n",
    "            timestamp >= date_trunc('week', current_date() - interval 1 week) \n",
    "            AND timestamp < date_trunc('week', current_date())\n",
    "    \"\"\"\n",
    "    logger.info(\"Executing query for average observed temperature last week...\")\n",
    "    result_temperature = spark.sql(query).collect()[0][0]\n",
    "    logger.info(f\"Average observed temperature for last week(Mon-Sun) is: {result_temperature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bbfb908-5d91-4bad-88cd-cef2dd191638",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "def get_max_wind_speed_change_last_7_days(full_table_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Calculate the maximum wind speed change between two consecutive observations \n",
    "    in the last 7 days.\n",
    "    \n",
    "    Args:\n",
    "        full_table_name (str): The fully qualified table name (e.g., \"catalog.schema.table\").\n",
    "        \n",
    "    Returns:\n",
    "        None: Displays the results directly.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "        WITH ranked_data AS (\n",
    "            SELECT\n",
    "                *,\n",
    "                LAG(wind_speed) OVER (PARTITION BY station_id ORDER BY timestamp) AS prev_wind_speed\n",
    "            FROM {full_table_name}\n",
    "            WHERE timestamp >= date_sub(current_date(), 7)\n",
    "        )\n",
    "        SELECT \n",
    "            MAX(ABS(wind_speed - prev_wind_speed)) AS max_wind_speed_change\n",
    "        FROM ranked_data\n",
    "        WHERE prev_wind_speed IS NOT NULL\n",
    "    \"\"\"\n",
    "    logger.info(\"Executing query for maximum wind speed change last 7 days...\")\n",
    "    result_wind = spark.sql(query).collect()[0][0]\n",
    "    logger.info(f\"Maximum wind speed change last 7 days is: {result_wind}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d53dd76-3d2e-4bae-8af5-2dcd288cc4d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Main Function\n",
    "\n",
    "The main function orchestrates the pipeline by:\n",
    "\n",
    "1. Fetching weather data for the last 7 days.\n",
    "2. Transforming the data into a structured format.\n",
    "3. Upserting the transformed data into the Delta table.\n",
    "4. Executing the SQL queries to compute the required metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2f98192-9056-4523-8690-ee357d8354b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\nINFO:WeatherDataPipeline:Pipeline executed successfully.\nINFO:WeatherDataPipeline:Executing query for average observed temperature last week...\nINFO:WeatherDataPipeline:Average observed temperature for last week(Mon-Sun) is: 13.309327731092434\nINFO:WeatherDataPipeline:Executing query for maximum wind speed change last 7 days...\nINFO:WeatherDataPipeline:Maximum wind speed change last 7 days is: 17.709999999999997\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    metadata = fetch_station_metadata(STATION_ID)\n",
    "    if metadata:\n",
    "        observations = fetch_weather_data(STATION_ID)\n",
    "        if observations:\n",
    "            df = transform_data_to_dataframe(observations, metadata)\n",
    "            upsert_to_table(df, FULL_TABLE_NAME)\n",
    "            logger.info(\"Pipeline executed successfully.\")\n",
    "            get_average_temperature_last_week(FULL_TABLE_NAME)\n",
    "            get_max_wind_speed_change_last_7_days(FULL_TABLE_NAME)\n",
    "        else:\n",
    "            logger.warning(\"No observations found.\")\n",
    "    else:\n",
    "        logger.warning(\"Failed to retrieve station metadata.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1436831711931620,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "challenge",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
