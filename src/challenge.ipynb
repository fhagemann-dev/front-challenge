{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "465757fb-b2c2-4d7e-af39-129e7fa88a0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import logging\n",
    "from typing import Any, Dict, List, Optional\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "from pyspark.sql.functions import col, to_timestamp, current_timestamp, date_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9bdb05a-cbd2-41e0-97ec-23b0f8f8907d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"WeatherDataPipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d27de89-f9af-4bfc-871f-26150ce54a00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "HEADERS = {\"User-Agent\": \"(myweatherapp.com, contact@myweatherapp.com)\"}\n",
    "STATION_ID = \"0128W\"\n",
    "BASE_URL = \"https://api.weather.gov\"\n",
    "OBSERVATIONS_ENDPOINT = f\"{BASE_URL}/stations/{STATION_ID}/observations\"\n",
    "STATION_ENDPOINT = f\"{BASE_URL}/stations/{STATION_ID}\"\n",
    "CATALOG_NAME = \"company_data\"\n",
    "SCHEMA_NAME = \"weather\"\n",
    "TABLE_NAME = \"station_observations\"\n",
    "FULL_TABLE_NAME = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{TABLE_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74de18cc-e015-4a54-ab9d-1f07b70baad5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "def fetch_station_metadata(station_id: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fetch metadata for a given station, including timezone.\n",
    "    \n",
    "    Args:\n",
    "        station_id (str): The station ID to fetch metadata for.\n",
    "        \n",
    "    Returns:\n",
    "        Optional[Dict[str, Any]]: A dictionary containing metadata or None if unavailable.\n",
    "    \"\"\"\n",
    "    station_url = f\"{BASE_URL}/stations/{station_id}\"\n",
    "    try:\n",
    "        response = requests.get(station_url, headers=HEADERS)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            return {\n",
    "                \"station_id\": station_id,\n",
    "                \"station_name\": data.get(\"properties\", {}).get(\"name\", None),\n",
    "                \"timezone\": data.get(\"properties\", {}).get(\"timeZone\", None)\n",
    "            }\n",
    "        else:\n",
    "            logger.warning(f\"Failed to fetch metadata for station {station_id}. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching metadata for station {station_id}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0817dcf2-0685-4ab5-9742-482fcae973c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def fetch_weather_data(station_id: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fetch weather observation data for a given station.\n",
    "    \n",
    "    Args:\n",
    "        station_id (str): The station ID to fetch data for.\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: A list of observation dictionaries.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(OBSERVATIONS_ENDPOINT, headers=HEADERS)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            return data.get(\"features\", [])\n",
    "        else:\n",
    "            logger.warning(f\"Failed to fetch observations for station {station_id}. Status code: {response.status_code}\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching observations for station {station_id}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d35d6c19-6435-42af-90a3-f71e7b94eab3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "def transform_data_to_dataframe(observations: List[Dict[str, Any]], metadata: Dict[str, Any]) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transform observation data and metadata into a PySpark DataFrame.\n",
    "\n",
    "    Args:\n",
    "        observations (List[Dict[str, Any]]): A list of observation dictionaries.\n",
    "        metadata (Dict[str, Any]): Metadata containing timezone and station details.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A PySpark DataFrame.\n",
    "    \"\"\"\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField(\"station_id\", StringType(), True),\n",
    "            StructField(\"station_name\", StringType(), True),\n",
    "            StructField(\"timezone\", StringType(), True),\n",
    "            StructField(\"latitude\", DoubleType(), True),\n",
    "            StructField(\"longitude\", DoubleType(), True),\n",
    "            StructField(\"timestamp_str\", StringType(), True),  # Store as String initially\n",
    "            StructField(\"temperature\", DoubleType(), True),\n",
    "            StructField(\"wind_speed\", DoubleType(), True),\n",
    "            StructField(\"humidity\", DoubleType(), True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    def safe_round(value: Any, decimals: int = 2) -> Optional[float]:\n",
    "        return round(float(value), decimals) if value is not None else None\n",
    "\n",
    "    records = []\n",
    "    for observation in observations:\n",
    "        props = observation.get(\"properties\", {})\n",
    "        coords = observation.get(\"geometry\", {}).get(\"coordinates\", [None, None])\n",
    "        records.append(\n",
    "            {\n",
    "                \"station_id\": metadata[\"station_id\"],\n",
    "                \"station_name\": metadata[\"station_name\"],\n",
    "                \"timezone\": metadata[\"timezone\"],\n",
    "                \"latitude\": coords[1],\n",
    "                \"longitude\": coords[0],\n",
    "                \"timestamp_str\": props.get(\"timestamp\"),  # Keep as raw ISO-8601 string\n",
    "                \"temperature\": safe_round(props.get(\"temperature\", {}).get(\"value\")),\n",
    "                \"wind_speed\": safe_round(props.get(\"windSpeed\", {}).get(\"value\")),\n",
    "                \"humidity\": safe_round(props.get(\"relativeHumidity\", {}).get(\"value\")),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df = spark.createDataFrame(records, schema=schema)\n",
    "    # Convert ISO-8601 string to TimestampType, and drop the original\n",
    "    df = df.withColumn(\"timestamp\", to_timestamp(col(\"timestamp_str\")))\n",
    "    df = df.drop(\"timestamp_str\")\n",
    "    # Filter for only the last 7 days\n",
    "    df = df.filter(col(\"timestamp\") >= date_sub(current_timestamp(), 7))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8157bae-6f21-471e-95a0-18a6b25eff6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "def upsert_to_table(dataframe: DataFrame, table_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Upsert data into a Unity Catalog table using MERGE.\n",
    "    \n",
    "    Args:\n",
    "        dataframe (DataFrame): The DataFrame to upsert.\n",
    "        table_name (str): The table name.\n",
    "    \"\"\"\n",
    "    dataframe.createOrReplaceTempView(\"temp_observations\")\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            station_id STRING,\n",
    "            station_name STRING,\n",
    "            timezone STRING,\n",
    "            latitude DOUBLE,\n",
    "            longitude DOUBLE,\n",
    "            timestamp TIMESTAMP,\n",
    "            temperature DOUBLE,\n",
    "            wind_speed DOUBLE,\n",
    "            humidity DOUBLE\n",
    "        )\n",
    "        USING delta\n",
    "    \"\"\")\n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO {table_name} target\n",
    "        USING temp_observations source\n",
    "        ON target.station_id = source.station_id AND target.timestamp = source.timestamp\n",
    "        WHEN MATCHED THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2f98192-9056-4523-8690-ee357d8354b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\nINFO:WeatherDataPipeline:Pipeline executed successfully.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    metadata = fetch_station_metadata(STATION_ID)\n",
    "    if metadata:\n",
    "        observations = fetch_weather_data(STATION_ID)\n",
    "        if observations:\n",
    "            df = transform_data_to_dataframe(observations, metadata)\n",
    "            upsert_to_table(df, FULL_TABLE_NAME)\n",
    "            logger.info(\"Pipeline executed successfully.\")\n",
    "        else:\n",
    "            logger.warning(\"No observations found.\")\n",
    "    else:\n",
    "        logger.warning(\"Failed to retrieve station metadata.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 54830810833718,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "challenge",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
